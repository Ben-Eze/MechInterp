{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "492545b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from einops import rearrange, reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f2bd25bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "# simple function\n",
    "def mod_addition(a, b, P):\n",
    "    return (a + b) % P\n",
    "\n",
    "P = 13\n",
    "a = 6\n",
    "b = 11\n",
    "\n",
    "print(mod_addition(a, b, P))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5556e366",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.9075, -1.0094,  1.0857, -1.1346,  1.1547, -1.1456,  1.1075, -1.0413,\n",
      "         0.9488, -0.8322,  0.6946, -0.5394,  0.3705])\n",
      "tensor([4])\n"
     ]
    }
   ],
   "source": [
    "# complex algorithm\n",
    "def mod_addition_alg(a, b, P):\n",
    "    w = torch.tensor([14, 35, 41, 42, 52])                          # frequencies\n",
    "    c = rearrange(torch.arange(P), \"x -> x 1\")                      # potential c values\n",
    "    cos = torch.cos;    sin = torch.sin                             # to simplify following expression\n",
    "\n",
    "    terms = (cos(w*a)*cos(w*b) - sin(w*a)*sin(w*b)) * cos(w*c) \\\n",
    "        + (sin(w*a)*cos(w*b) - cos(w*a)*sin(w*b)) * sin(w*c)        # trig terms, expanded out\n",
    "    logits = reduce(terms, \"h w -> h\", \"sum\")                       # sum across the frequencies\n",
    "    print(logits)\n",
    "\n",
    "    i_max = torch.argmax(logits).item()\n",
    "    return c[i_max]\n",
    "\n",
    "P = 13\n",
    "a = 6\n",
    "b = 11\n",
    "\n",
    "print(mod_addition_alg(a, b, P))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "692f97f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1d4ffccd550>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d3600a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wei: tensor([[0.5000, 0.5000],\n",
      "        [0.5000, 0.5000]], grad_fn=<SelectBackward0>)\n",
      "Q: tensor([[-0.1264, -0.0083,  0.0090, -0.9598, -0.5485,  0.5829, -0.9033,  0.1759,\n",
      "         -1.4082,  0.5907, -0.0114,  0.1595,  0.6726,  0.2361, -0.2484, -0.2638,\n",
      "         -0.2969,  0.5122,  0.0926, -0.1770,  0.4638, -0.8553,  0.3490, -0.8564,\n",
      "         -1.4068, -1.3824, -0.6120,  0.5088,  0.1498,  0.0990,  0.5465, -0.9410],\n",
      "        [-0.1264, -0.0083,  0.0090, -0.9598, -0.5485,  0.5829, -0.9033,  0.1759,\n",
      "         -1.4082,  0.5907, -0.0114,  0.1595,  0.6726,  0.2361, -0.2484, -0.2638,\n",
      "         -0.2969,  0.5122,  0.0926, -0.1770,  0.4638, -0.8553,  0.3490, -0.8564,\n",
      "         -1.4068, -1.3824, -0.6120,  0.5088,  0.1498,  0.0990,  0.5465, -0.9410]],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "K_transpose: tensor([[-6.8638e-01, -6.8638e-01],\n",
      "        [ 1.4519e-01,  1.4519e-01],\n",
      "        [-1.0565e+00, -1.0565e+00],\n",
      "        [-1.1058e+00, -1.1058e+00],\n",
      "        [-4.7714e-01, -4.7714e-01],\n",
      "        [-3.1765e-01, -3.1765e-01],\n",
      "        [-4.7249e-01, -4.7249e-01],\n",
      "        [ 1.0852e+00,  1.0852e+00],\n",
      "        [-3.1530e-01, -3.1530e-01],\n",
      "        [-2.4604e-03, -2.4604e-03],\n",
      "        [ 1.2287e-01,  1.2287e-01],\n",
      "        [ 6.7682e-01,  6.7682e-01],\n",
      "        [-8.7006e-01, -8.7006e-01],\n",
      "        [-3.3169e-01, -3.3169e-01],\n",
      "        [-1.8192e-01, -1.8192e-01],\n",
      "        [ 1.3488e-01,  1.3488e-01],\n",
      "        [ 3.1600e-01,  3.1600e-01],\n",
      "        [-9.5423e-01, -9.5423e-01],\n",
      "        [ 2.7499e-01,  2.7499e-01],\n",
      "        [-5.2520e-04, -5.2520e-04],\n",
      "        [-6.9081e-02, -6.9081e-02],\n",
      "        [-4.8574e-01, -4.8574e-01],\n",
      "        [ 7.2678e-01,  7.2678e-01],\n",
      "        [-4.8396e-01, -4.8396e-01],\n",
      "        [-2.1850e-01, -2.1850e-01],\n",
      "        [ 8.1557e-03,  8.1557e-03],\n",
      "        [-2.8734e-01, -2.8734e-01],\n",
      "        [-2.6116e-01, -2.6116e-01],\n",
      "        [-1.6531e-01, -1.6531e-01],\n",
      "        [-7.7753e-02, -7.7753e-02],\n",
      "        [-3.4486e-01, -3.4486e-01],\n",
      "        [-5.1521e-01, -5.1521e-01]], grad_fn=<SelectBackward0>)\n",
      "wei: tensor([[0.5000, 0.5000],\n",
      "        [0.5000, 0.5000]], grad_fn=<SelectBackward0>)\n",
      "Q: tensor([[-0.1264, -0.0083,  0.0090, -0.9598, -0.5485,  0.5829, -0.9033,  0.1759,\n",
      "         -1.4082,  0.5907, -0.0114,  0.1595,  0.6726,  0.2361, -0.2484, -0.2638,\n",
      "         -0.2969,  0.5122,  0.0926, -0.1770,  0.4638, -0.8553,  0.3490, -0.8564,\n",
      "         -1.4068, -1.3824, -0.6120,  0.5088,  0.1498,  0.0990,  0.5465, -0.9410],\n",
      "        [-0.1264, -0.0083,  0.0090, -0.9598, -0.5485,  0.5829, -0.9033,  0.1759,\n",
      "         -1.4082,  0.5907, -0.0114,  0.1595,  0.6726,  0.2361, -0.2484, -0.2638,\n",
      "         -0.2969,  0.5122,  0.0926, -0.1770,  0.4638, -0.8553,  0.3490, -0.8564,\n",
      "         -1.4068, -1.3824, -0.6120,  0.5088,  0.1498,  0.0990,  0.5465, -0.9410]],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "K_transpose: tensor([[-6.8638e-01, -6.8638e-01],\n",
      "        [ 1.4519e-01,  1.4519e-01],\n",
      "        [-1.0565e+00, -1.0565e+00],\n",
      "        [-1.1058e+00, -1.1058e+00],\n",
      "        [-4.7714e-01, -4.7714e-01],\n",
      "        [-3.1765e-01, -3.1765e-01],\n",
      "        [-4.7249e-01, -4.7249e-01],\n",
      "        [ 1.0852e+00,  1.0852e+00],\n",
      "        [-3.1530e-01, -3.1530e-01],\n",
      "        [-2.4604e-03, -2.4604e-03],\n",
      "        [ 1.2287e-01,  1.2287e-01],\n",
      "        [ 6.7682e-01,  6.7682e-01],\n",
      "        [-8.7006e-01, -8.7006e-01],\n",
      "        [-3.3169e-01, -3.3169e-01],\n",
      "        [-1.8192e-01, -1.8192e-01],\n",
      "        [ 1.3488e-01,  1.3488e-01],\n",
      "        [ 3.1600e-01,  3.1600e-01],\n",
      "        [-9.5423e-01, -9.5423e-01],\n",
      "        [ 2.7499e-01,  2.7499e-01],\n",
      "        [-5.2520e-04, -5.2520e-04],\n",
      "        [-6.9081e-02, -6.9081e-02],\n",
      "        [-4.8574e-01, -4.8574e-01],\n",
      "        [ 7.2678e-01,  7.2678e-01],\n",
      "        [-4.8396e-01, -4.8396e-01],\n",
      "        [-2.1850e-01, -2.1850e-01],\n",
      "        [ 8.1557e-03,  8.1557e-03],\n",
      "        [-2.8734e-01, -2.8734e-01],\n",
      "        [-2.6116e-01, -2.6116e-01],\n",
      "        [-1.6531e-01, -1.6531e-01],\n",
      "        [-7.7753e-02, -7.7753e-02],\n",
      "        [-3.4486e-01, -3.4486e-01],\n",
      "        [-5.1521e-01, -5.1521e-01]], grad_fn=<SelectBackward0>)\n",
      "wei: tensor([[0.5000, 0.5000],\n",
      "        [0.5000, 0.5000]], grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Model architecture\n",
    "# \"one-layer ReLU transformer, token embeddings with d = 128, learned positional embeddings, 4 attention heads of dimension d/4 = 32, and n = 512 hidden units in the MLP. In other experiments, we vary the depth and dimension of the model. We did not use LayerNorm or tie our embed/unembed matrices.\"\n",
    "\n",
    "class SelfAttentionHead(nn.Module):\n",
    "    def __init__(self, d_token, d_head, l_context):\n",
    "        super().__init__()\n",
    "\n",
    "        self.d_head = d_head\n",
    "\n",
    "        self.key = nn.Linear(d_token, d_head, bias=False)\n",
    "        self.query = nn.Linear(d_token, d_head, bias=False)\n",
    "        self.value = nn.Linear(d_token, d_head, bias=False)\n",
    "        self.softmax = nn.Softmax(1)\n",
    "        \n",
    "        self.register_buffer(\"tril\", torch.tril(torch.ones(l_context, l_context))) # TODO: delete\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x:        (b, t, d_token)\n",
    "        # K, Q, V:  (b, t, d_head)\n",
    "        # Q @ K':   (b, t, t)\n",
    "        _, T, _ = x.shape\n",
    "\n",
    "        K_transpose = rearrange(self.key(x), \"b t d_head -> b d_head t\")\n",
    "        Q = self.query(x)\n",
    "        V = self.value(x)\n",
    "\n",
    "        wei = self.softmax(\n",
    "            ((Q @ K_transpose) / (self.d_head)**0.5).masked_fill(\n",
    "                self.tril[:T, :T] == 0, -torch.inf)\n",
    "        ) \n",
    "        attention = wei @ V\n",
    "\n",
    "        return attention\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, l_context, d_token, N_heads, d_head):\n",
    "        super().__init__()\n",
    "        self.N_heads = N_heads\n",
    "        self.d_head = d_head\n",
    "\n",
    "        self.sa_heads = nn.ModuleList(\n",
    "            [SelfAttentionHead(d_token, d_head, l_context) for _ in range(self.N_heads)]\n",
    "        )\n",
    "\n",
    "        # projects from concatenated sa_head outputs to something to add to the residual stream\n",
    "        self.proj = nn.Linear(N_heads*d_head, d_token) \n",
    "\n",
    "    def forward(self, x):\n",
    "        x_concat = torch.concat([sa(x) for sa in self.sa_heads], dim=2)\n",
    "        x += self.proj(x_concat)\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, l_context, d_token, \n",
    "                 N_heads, d_head, \n",
    "                 d_ffwd):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.sa_heads = MultiHeadAttention(l_context, d_token, N_heads, d_head)\n",
    "\n",
    "        self.ln1 = nn.LayerNorm(d_token)\n",
    "\n",
    "        self.ffwd_head = nn.Sequential(\n",
    "            nn.Linear(d_token, d_ffwd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(d_ffwd, d_token)\n",
    "        )\n",
    "\n",
    "        self.ln2 = nn.LayerNorm(d_token)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x += self.ln1(self.sa_heads(x))\n",
    "        x += self.ln2(self.ffwd_head(x))\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class ModAdd(torch.nn.Module):\n",
    "    def __init__(self, p, \n",
    "                 d,\n",
    "                 N_heads, d_head,\n",
    "                 d_ffwd, \n",
    "                 n):\n",
    "        \"\"\"\n",
    "        model = ModAdd(p, ...)\n",
    "        c = model(a, b)\n",
    "        -> c = (a + b) % p\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.d_inout = p                # prime number on the RHS of '%'\n",
    "        # Residual stream\n",
    "        self.d_token = d                # embedding dimension of tokens\n",
    "        self.l_context = 2              # context length\n",
    "        # Attention heads\n",
    "        self.N_heads = N_heads          # no. of sa_heads working in parallel\n",
    "        self.d_head = d_head            # dimension of each sa_head (ie. final dim of K, Q, V arrays)\n",
    "        # Feed-forward layer (within block)\n",
    "        self.d_ffwd = d_ffwd\n",
    "        # Linear layer TODO: delete\n",
    "        # self.d_linear = n\n",
    "\n",
    "        # Layers\n",
    "        self.token_embedding_table = nn.Embedding(\n",
    "            num_embeddings=self.d_inout, \n",
    "            embedding_dim=self.d_token\n",
    "        )\n",
    "        \n",
    "        self.block = Block(\n",
    "            self.l_context, self.d_token, \n",
    "            self.N_heads, self.d_head, \n",
    "            self.d_ffwd)\n",
    "\n",
    "        self.linear = nn.Linear(self.d_token, self.d_inout)\n",
    "\n",
    "        self.cross_entropy = nn.CrossEntropyLoss()\n",
    "        \n",
    "    def forward(self, X, X_target=None):\n",
    "        x = self.token_embedding_table(X)           # no positional encoding needed\n",
    "        x_ffwd = self.block(x)\n",
    "        logits = self.linear(x_ffwd)\n",
    "\n",
    "        if X_target is None:\n",
    "            loss = None\n",
    "            return logits, loss\n",
    "        \n",
    "        B, T, C = logits.shape\n",
    "        assert (B == 1) & (C == self.d_inout)\n",
    "        loss = self.cross_entropy(logits.view(B))\n",
    "        \n",
    "        return logits, loss\n",
    "\n",
    "p = 7\n",
    "model = ModAdd(p=p, d=128, N_heads=4, d_head=32, d_ffwd=128*4, n=512).to(device)\n",
    "\n",
    "X = torch.randint(0, p, [5, 2])\n",
    "Y_pred = model(X)\n",
    "print(Y_pred.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "124c69f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Changes:\n",
    "- redo self-attention\n",
    "    -> no need for 'attention' part of transformer? (since position isn't important)\n",
    "       (maybe make it easy to turn on and off or at least have a think about it)\n",
    "    -> actually gonna be simpler because there's only ever one new token and it has (a,b) as context (no need for tril mumbo jumbo)\n",
    "- simplify the loss calculation\n",
    "    -> the output is a (..., 1) vector (ie. c), so that might make it a bit simpler...? (not really, but just need to implement)\n",
    "- sample the logits (just argmax for accuracy)\n",
    "\n",
    "TODO:\n",
    "- train it\n",
    "- look at Nander's library\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f16becf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.3333, 0.0000, 0.0000],\n",
      "        [0.5000, 0.5000, 0.0000],\n",
      "        [1.0000, 1.0000, 1.0000]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(4, 3)\n",
    "wei = torch.tril(torch.ones(3, 3))\n",
    "wei /= reduce(wei, \"i j -> j 1\", \"sum\")\n",
    "print(wei)\n",
    "xbow = wei @ x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "49f98796",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data\n",
    "# 30% training, 70% test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "816b18ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "# full-batch gradient descent\n",
    "# AdamW optimiser\n",
    "# learning rage = 0.001, \n",
    "# N_epochs = 40_000"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
